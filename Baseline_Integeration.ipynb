{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm.sklearn import LGBMClassifier\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, log_loss, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy import sparse\n",
    "from scipy.stats import kurtosis\n",
    "import time\n",
    "import warnings\n",
    "import lightgbm as lgb\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   user_id  listing_id auditing_date  term  rate  principal  \\\n",
      "0   316610     1556649    2017-11-26     9   7.6       4800   \n",
      "1    62002     1556633    2017-11-26     6   7.6       4000   \n",
      "2   192135     1556629    2017-11-26    12   8.0       8660   \n",
      "3   487382     1556628    2017-11-26     9   7.6       4780   \n",
      "4   235186     1556627    2017-11-26     9   7.6       1480   \n",
      "\n",
      "   auditing_date_month  auditing_date_days  principal_per_term  \\\n",
      "0                   11                  30          533.333333   \n",
      "1                   11                  30          666.666667   \n",
      "2                   11                  30          721.666667   \n",
      "3                   11                  30          531.111111   \n",
      "4                   11                  30          164.444444   \n",
      "\n",
      "   principal_class  TermRate  \n",
      "0                2  0.844444  \n",
      "1                2  1.266667  \n",
      "2                1  0.666667  \n",
      "3                2  0.844444  \n",
      "4                5  0.844444  \n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv('dataset/train.csv', parse_dates=['auditing_date', 'due_date', 'repay_date'])\n",
    "train_df['repay_date'] = train_df[['due_date', 'repay_date']].apply(lambda x: x['repay_date'] if x['repay_date'] != '\\\\N' else x['due_date'], axis=1)\n",
    "train_df['repay_amt'] = train_df['repay_amt'].apply(lambda x: x if x != '\\\\N' else 0).astype('float32')\n",
    "train_df['label'] = (train_df['due_date'] - train_df['repay_date']).dt.days\n",
    "train_df.loc[train_df['repay_amt'] == 0, 'label'] = 32\n",
    "clf_labels = train_df['label'].values\n",
    "amt_labels = train_df['repay_amt'].values\n",
    "del train_df['label'], train_df['repay_amt'], train_df['repay_date']\n",
    "train_due_amt_df = train_df[['due_amt']]\n",
    "train_num = train_df.shape[0]\n",
    "test_df = pd.read_csv('dataset/test.csv', parse_dates=['auditing_date', 'due_date'])\n",
    "sub = test_df[['listing_id', 'auditing_date', 'due_amt']]\n",
    "df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "\n",
    "listing_info_df = pd.read_csv('dataset/listing_info.csv')\n",
    "listing_info_df['auditing_date_month'] = pd.to_datetime(listing_info_df['auditing_date']).dt.month\n",
    "listing_info_df['auditing_date_days'] = listing_info_df['auditing_date_month'].copy()\n",
    "# print(sub_example['due_date'])\n",
    "listing_info_df['auditing_date_days'][listing_info_df['auditing_date_days']==1] = 31\n",
    "listing_info_df['auditing_date_days'][listing_info_df['auditing_date_days']==3] = 31\n",
    "listing_info_df['auditing_date_days'][listing_info_df['auditing_date_days']==5] = 31\n",
    "listing_info_df['auditing_date_days'][listing_info_df['auditing_date_days']==7] = 31\n",
    "listing_info_df['auditing_date_days'][listing_info_df['auditing_date_days']==8] = 31\n",
    "listing_info_df['auditing_date_days'][listing_info_df['auditing_date_days']==10] = 31\n",
    "listing_info_df['auditing_date_days'][listing_info_df['auditing_date_days']==12] = 31\n",
    "listing_info_df['auditing_date_days'][listing_info_df['auditing_date_days']==4] = 30\n",
    "listing_info_df['auditing_date_days'][listing_info_df['auditing_date_days']==6] = 30\n",
    "listing_info_df['auditing_date_days'][listing_info_df['auditing_date_days']==9] = 30\n",
    "listing_info_df['auditing_date_days'][listing_info_df['auditing_date_days']==11] = 30\n",
    "listing_info_df['auditing_date_days'][listing_info_df['auditing_date_days']==2] = 28\n",
    "\n",
    "listing_info_df['principal_per_term'] = listing_info_df['principal']/listing_info_df['term']\n",
    "list_count=listing_info_df['user_id'].value_counts()\n",
    "df_count_times=pd.DataFrame(list_count)\n",
    "df_count_times.rename(columns={ df_count_times.columns[0]: \"loan_times\" },inplace=True)\n",
    "df_count_times['user_id']=df_count_times.index\n",
    "bin_size=(1,2,4,8)\n",
    "def count_to_style(count):\n",
    "    n=1\n",
    "    num=1\n",
    "    while count>n and num<5:\n",
    "        n=n*2\n",
    "        num=num+1\n",
    "    return num\n",
    "df_count_times['loan_times_type']=df_count_times['loan_times'].apply(count_to_style)\n",
    "del df_count_times['loan_times']\n",
    "\n",
    "\n",
    "list_principle=listing_info_df['principal'].value_counts()\n",
    "principal_df=pd.DataFrame(list_principle)\n",
    "principal_df.rename(columns={principal_df.columns[0]: \"frequency\"},inplace=True)\n",
    "principal_df['principal']=principal_df.index\n",
    "principal_df.reset_index(drop=True, inplace=True)\n",
    "def ChangeToClass(principal):\n",
    "    if principal<=1170:\n",
    "        return 6\n",
    "    elif 1170<principal<=2260:\n",
    "        return 5\n",
    "    elif 2260<principal<=3350:\n",
    "        return 4\n",
    "    elif 3350<principal<=3910:\n",
    "        return 3\n",
    "    elif 3910<principal<=5540:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1\n",
    "principal_df['principal_class']=principal_df['principal'].apply(ChangeToClass)\n",
    "del principal_df['frequency']\n",
    "\n",
    "listing_info_df=listing_info_df.merge(principal_df, on=\"principal\", how=\"left\")\n",
    "\n",
    "listing_info_df['TermRate']=listing_info_df['rate']/listing_info_df['term']\n",
    "\n",
    "print(listing_info_df.head())\n",
    "del listing_info_df['user_id'], listing_info_df['auditing_date']\n",
    "df = df.merge(listing_info_df, on='listing_id', how='left')\n",
    "\n",
    "# 表中有少数user不止一条记录，因此按日期排序，去重，只保留最新的一条记录。\n",
    "user_info = pd.read_csv('dataset/user_info.csv', parse_dates=['reg_mon', 'insertdate'])\n",
    "\n",
    "user_info.rename(columns={'insertdate': 'info_insert_date'}, inplace=True)\n",
    "\n",
    "user_info = user_info.sort_values(by='info_insert_date', ascending=False).drop_duplicates('user_id').reset_index(drop=True)\n",
    "\n",
    "user_info['gender']=user_info['gender'].apply(lambda x:1 if x.strip()=='男' else 0)\n",
    "\n",
    "user_info['id_city'] = user_info['id_city'].apply(lambda x: x.replace('\\\\N', 'c0'))\n",
    "def compare(a,b):\n",
    "    if a==b: return 1\n",
    "    else: return 0\n",
    "user_info['remote_boolean']=user_info.apply(lambda user_info:compare(user_info['cell_province'],user_info['id_province']),axis=1)\n",
    "\n",
    "city_map=user_info.groupby('id_city').agg({'id_city':'count'})\n",
    "city_map[city_map['id_city']>11000]\n",
    "city_map.columns=['count']\n",
    "city_map.reset_index(inplace=True)\n",
    "x = city_map[city_map[\"count\"] < 11000]; \n",
    "max_count = city_map[\"count\"].max()\n",
    "min_count = city_map[\"count\"].min()\n",
    "\n",
    "bin_size = int((max_count - min_count) / 5); \n",
    "def count_to_class(count): \n",
    "    return min(6, int((count - min_count) / bin_size))+1\n",
    "city_map['class']=city_map['count'].apply(count_to_class)\n",
    "city_map[\"class\"][0]=0\n",
    "city_map = city_map[[\"id_city\", \"class\"]]\n",
    "user_info=user_info.merge(city_map, on=\"id_city\", how=\"left\")\n",
    "\n",
    "user_info=user_info.merge(df_count_times, on=\"user_id\", how=\"left\")  # 合并贷款次数到user_info里面\n",
    "\n",
    "df = df.merge(user_info, on='user_id', how='left')\n",
    "\n",
    "# 同上\n",
    "user_tag_df = pd.read_csv('dataset/user_taglist.csv', parse_dates=['insertdate'])\n",
    "user_tag_df.rename(columns={'insertdate': 'tag_insert_date'}, inplace=True)\n",
    "user_tag_df = user_tag_df.sort_values(by='tag_insert_date', ascending=False).drop_duplicates('user_id').reset_index(drop=True)\n",
    "df = df.merge(user_tag_df, on='user_id', how='left')\n",
    "\n",
    "# 历史记录表能做的特征远不止这些\n",
    "repay_log_df = pd.read_csv('dataset/user_repay_logs.csv', parse_dates=['due_date', 'repay_date'])\n",
    "\n",
    "# 由于题目任务只预测第一期的还款情况，因此这里只保留第一期的历史记录。当然非第一期的记录也能提取很多特征。\n",
    "repay_log_df = repay_log_df[repay_log_df['order_id'] == 1].reset_index(drop=True)\n",
    "repay_log_df['repay'] = repay_log_df['repay_date'].astype('str').apply(lambda x: 1 if x != '2200-01-01' else 0)\n",
    "repay_log_df['early_repay_days'] = (repay_log_df['due_date'] - repay_log_df['repay_date']).dt.days\n",
    "repay_log_df['early_repay_days'] = repay_log_df['early_repay_days'].apply(lambda x: x if x >= 0 else -1)\n",
    "for f in ['listing_id', 'order_id', 'due_date', 'repay_date', 'repay_amt']:\n",
    "    del repay_log_df[f]\n",
    "\n",
    "group = repay_log_df.groupby('user_id', as_index=False)\n",
    "repay_log_df = repay_log_df.merge(group['repay'].agg({'repay_mean': 'mean'}), on='user_id', how='left')\n",
    "\n",
    "repay_log_df = repay_log_df.merge(\n",
    "    group['early_repay_days'].agg({\n",
    "        'early_repay_days_max': 'max', 'early_repay_days_median': 'median', 'early_repay_days_sum': 'sum',\n",
    "        'early_repay_days_mean': 'mean', 'early_repay_days_std': 'std'\n",
    "    }), on='user_id', how='left'\n",
    ")\n",
    "\n",
    "repay_log_df = repay_log_df.merge(\n",
    "    group['due_amt'].agg({\n",
    "        'due_amt_max': 'max', 'due_amt_min': 'min', 'due_amt_median': 'median',\n",
    "        'due_amt_mean': 'mean', 'due_amt_sum': 'sum', 'due_amt_std': 'std',\n",
    "        'due_amt_skew': 'skew', 'due_amt_kurt': kurtosis, 'due_amt_ptp': np.ptp\n",
    "    }), on='user_id', how='left'\n",
    ")\n",
    "del repay_log_df['repay'], repay_log_df['early_repay_days'], repay_log_df['due_amt']\n",
    "repay_log_df = repay_log_df.drop_duplicates('user_id').reset_index(drop=True)\n",
    "df = df.merge(repay_log_df, on='user_id', how='left')\n",
    "\n",
    "repay_log_df_all = pd.read_csv('dataset/user_repay_logs.csv', parse_dates=['due_date', 'repay_date'])\n",
    "repay_log_df_all['repay_all_order'] = repay_log_df_all['repay_date'].astype('str').apply(lambda x: 1 if x != '2200-01-01' else 0)\n",
    "repay_log_df_all['early_repay_days_all_order'] = (repay_log_df_all['due_date'] - repay_log_df_all['repay_date']).dt.days\n",
    "repay_log_df_all['early_repay_days_all_order'] = repay_log_df_all['early_repay_days_all_order'].apply(lambda x: x if x >= 0 else -1)\n",
    "for f in ['listing_id', 'order_id', 'due_date', 'repay_date', 'repay_amt']:\n",
    "    del repay_log_df_all[f]\n",
    "\n",
    "group = repay_log_df_all.groupby('user_id', as_index=False)\n",
    "repay_log_df_all = repay_log_df_all.merge(group['repay_all_order'].agg({'repay_mean_all_order': 'mean'}), on='user_id', how='left')\n",
    "\n",
    "repay_log_df_all = repay_log_df_all.merge(\n",
    "    group['early_repay_days_all_order'].agg({\n",
    "        'early_repay_days_max_all_order': 'max', 'early_repay_days_median_all_order': 'median', 'early_repay_days_sum_all_order': 'sum',\n",
    "        'early_repay_days_mean_all_order': 'mean', 'early_repay_days_std_all_order': 'std'\n",
    "    }), on='user_id', how='left'\n",
    ")\n",
    "\n",
    "repay_log_df_all = repay_log_df_all.merge(\n",
    "    group['due_amt'].agg({\n",
    "        'due_amt_max_all_order': 'max', 'due_amt_min_all_order': 'min', 'due_amt_median_all_order': 'median',\n",
    "        'due_amt_mean_all_order': 'mean', 'due_amt_sum_all_order': 'sum', 'due_amt_std_all_order': 'std',\n",
    "        'due_amt_skew_all_order': 'skew', 'due_amt_kurt_all_order': kurtosis, 'due_amt_ptp_all_order': np.ptp\n",
    "    }), on='user_id', how='left'\n",
    ")\n",
    "del repay_log_df_all['repay_all_order'], repay_log_df_all['early_repay_days_all_order'], repay_log_df_all['due_amt']\n",
    "repay_log_df_all = repay_log_df_all.drop_duplicates('user_id').reset_index(drop=True)\n",
    "df = df.merge(repay_log_df_all, on='user_id', how='left')\n",
    "\n",
    "\n",
    "cate_cols = ['cell_province', 'id_province', 'id_city']\n",
    "for f in cate_cols:\n",
    "    df[f] = df[f].map(dict(zip(df[f].unique(), range(df[f].nunique())))).astype('int32')\n",
    "\n",
    "df['due_amt_per_days'] = df['due_amt'] / (train_df['due_date'] - train_df['auditing_date']).dt.days\n",
    "date_cols = ['auditing_date', 'due_date', 'reg_mon', 'info_insert_date', 'tag_insert_date']\n",
    "\n",
    "for f in date_cols:\n",
    "    if f in ['reg_mon', 'info_insert_date', 'tag_insert_date']:\n",
    "        df[f + '_year'] = df[f].dt.year\n",
    "    df[f + '_month'] = df[f].dt.month\n",
    "    if f in ['auditing_date', 'due_date', 'info_insert_date', 'tag_insert_date']:\n",
    "        df[f + '_day'] = df[f].dt.day\n",
    "        df[f + '_dayofweek'] = df[f].dt.dayofweek\n",
    "\n",
    "df.drop(columns=date_cols, axis=1, inplace=True)\n",
    "\n",
    "# df['taglist'] = df['taglist'].astype('str').apply(lambda x: x.strip().replace('|', ' ').strip())\n",
    "# tag_cv = CountVectorizer(min_df=10, max_df=0.9).fit_transform(df['taglist'])\n",
    "\n",
    "del df['user_id'], df['listing_id'], df['taglist']\n",
    "df_count_times\n",
    "df = pd.get_dummies(df, columns=cate_cols)\n",
    "# df = sparse.hstack((df.values, tag_cv), format='csr', dtype='float32')\n",
    "# train_values, test_values = df[:train_num].values, df[train_num:].values\n",
    "# print(train_values.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_values, test_values = df[:train_num].values, df[train_num:].values\n",
    "# 五折验证也可以改成一次验证，按时间划分训练集和验证集，以避免由于时序引起的数据穿越问题。\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2019)\n",
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'multiclass',\n",
    "    'metric': 'multi_logloss',\n",
    "    'metric_freq': 5,\n",
    "    'num_class': 33,\n",
    "    'num_iterations': 600,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "#     'device_type':'gpu'\n",
    "}\n",
    "amt_oof = np.zeros(train_num)\n",
    "prob_oof = np.zeros((train_num, 33))\n",
    "test_pred_prob = np.zeros((test_values.shape[0], 33))\n",
    "\n",
    "\n",
    "for i, (trn_idx, val_idx) in enumerate(skf.split(train_values, clf_labels)):\n",
    "    print(i, 'fold...')\n",
    "    t = time.time()\n",
    "    trn_x, trn_y = train_values[trn_idx], clf_labels[trn_idx]\n",
    "    val_x, val_y = train_values[val_idx], clf_labels[val_idx]\n",
    "    val_repay_amt = amt_labels[val_idx]\n",
    "    val_due_amt = train_due_amt_df.iloc[val_idx]\n",
    "   \n",
    "    train = lgb.Dataset(trn_x, label=trn_y)\n",
    "    val = lgb.Dataset(val_x, label=val_y, reference=train)\n",
    "    gbm = lgb.train(params, train, valid_sets=val, early_stopping_rounds=10)\n",
    "\n",
    "    # shepe = (-1, 33)\n",
    "    val_pred_prob_everyday = gbm.predict(val_x, num_iteration=gbm.best_iteration)\n",
    "    prob_oof[val_idx] = val_pred_prob_everyday\n",
    "    val_pred_prob_today = [val_pred_prob_everyday[i][val_y[i]] for i in range(val_pred_prob_everyday.shape[0])]\n",
    "    val_pred_repay_amt = val_due_amt['due_amt'].values * val_pred_prob_today\n",
    "    print('val rmse:', np.sqrt(mean_squared_error(val_repay_amt, val_pred_repay_amt)))\n",
    "    print('val mae:', mean_absolute_error(val_repay_amt, val_pred_repay_amt))\n",
    "    amt_oof[val_idx] = val_pred_repay_amt\n",
    "    test_pred_prob += gbm.predict(test_values, num_iteration=gbm.best_iteration) / skf.n_splits\n",
    "    print('runtime: {}\\n'.format(time.time() - t))\n",
    "\n",
    "print('\\ncv rmse:', np.sqrt(mean_squared_error(amt_labels, amt_oof)))\n",
    "print('cv mae:', mean_absolute_error(amt_labels, amt_oof))\n",
    "print('cv logloss:', log_loss(clf_labels, prob_oof))\n",
    "print('cv acc:', accuracy_score(clf_labels, np.argmax(prob_oof, axis=1)))\n",
    "\n",
    "prob_cols = ['prob_{}'.format(i) for i in range(33)]\n",
    "\n",
    "for i, f in enumerate(prob_cols):\n",
    "    sub[f] = test_pred_prob[:, i]\n",
    "\n",
    "sub_example = pd.read_csv('dataset/submission.csv', parse_dates=['repay_date'])\n",
    "\n",
    "sub_example = sub_example.merge(sub, on='listing_id', how='left')\n",
    "# sub_example['due_date'] = pd.to_datetime((sub_example['auditing_date'] + np.timedelta64(1, 'M') + np.timedelta64(1, 'D')).dt.date)\n",
    "sub_example['due_date'] = sub_example['auditing_date'].copy()\n",
    "# print(sub_example['due_date'])\n",
    "sub_example['due_date'][sub_example['due_date'].dt.month == 4] = pd.to_datetime((sub_example['due_date'][sub_example['due_date'].dt.month == 4] + np.timedelta64(1, 'M')).dt.date)\n",
    "sub_example['due_date'][sub_example['due_date'].dt.month == 3] = pd.to_datetime((sub_example['due_date'][sub_example['due_date'].dt.month == 3] + np.timedelta64(1, 'M') + np.timedelta64(1, 'D')).dt.date)\n",
    "sub_example['due_date'][sub_example['due_date'].dt.month == 2] = pd.to_datetime((sub_example['due_date'][sub_example['due_date'].dt.month == 2] + np.timedelta64(1, 'M') - np.timedelta64(2, 'D')).dt.date)\n",
    "\n",
    "sub_example['days'] = (sub_example['due_date'] - sub_example['repay_date']).dt.days\n",
    "\n",
    "# shape = (-1, 33)\n",
    "test_prob = sub_example[prob_cols].values\n",
    "test_labels = sub_example['days'].values\n",
    "test_prob = [test_prob[i][test_labels[i]] for i in range(test_prob.shape[0])]\n",
    "sub_example['repay_amt'] = sub_example['due_amt'] * test_prob\n",
    "sub_example[['listing_id', 'repay_date', 'repay_amt']].to_csv('sub_cross_5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 33)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
